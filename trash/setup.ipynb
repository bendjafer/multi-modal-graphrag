{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e16c42a5",
   "metadata": {},
   "source": [
    "### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c76e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Ignored the following yanked versions: 20081119\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement hashlib (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for hashlib\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install transformers torchvision\n",
    "#!pip install datasets evaluate accelerate timm\n",
    "#!pip install PyMuPDF\n",
    "#!pip install pdf2image\n",
    "#!pip install docling\n",
    "#!pip install matplotlib\n",
    "#!pip install seaborn\n",
    "#!pip install pandas\n",
    "#!pip install numpy\n",
    "#!pip install scikit-learn\n",
    "#!pip install hashlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3273640e",
   "metadata": {},
   "source": [
    "### Loging to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f439f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc715a892fc42b39d9421ca0ebaaebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "access_token = \"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\"\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be257f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# add openai clip model from transformers\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e91ec7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'gt_image_paths', 'question_type', 'answer_type', 'domain', 'longdoc_image_paths'],\n",
      "    num_rows: 200\n",
      "})\n",
      "{'question': 'In 2020, how much did the Maine Insurance Guaranty Association collect from each recovery category?', 'answer': 'Carrier reimbursement: $347,934, Large Deductible Reimbursement: $87, Supplement Benefit Fund: $38,779. In addition, the Association recovered $97,502 from early access distributions, NCCI settlement proceeds, and administrative expense reimbursements.', 'gt_image_paths': ['images/legal/3374686/3374686_page_0005.png', 'images/legal/3374686/3374686_page_0006.png', 'images/legal/3374686/3374686_page_0007.png'], 'question_type': 'factual_retrieval', 'answer_type': 'table_required', 'domain': 'legal', 'longdoc_image_paths': ['images/legal/3374686/3374686_page_0001.png', 'images/legal/3374686/3374686_page_0002.png', 'images/legal/3374686/3374686_page_0003.png', 'images/legal/3374686/3374686_page_0004.png', 'images/legal/3374686/3374686_page_0005.png', 'images/legal/3374686/3374686_page_0006.png', 'images/legal/3374686/3374686_page_0007.png', 'images/legal/3374686/3374686_page_0008.png', 'images/legal/3374686/3374686_page_0009.png', 'images/legal/3374686/3374686_page_0010.png', 'images/legal/3374686/3374686_page_0011.png', 'images/legal/3374686/3374686_page_0012.png', 'images/legal/3374686/3374686_page_0013.png', 'images/legal/3374686/3374686_page_0014.png', 'images/legal/3374686/3374686_page_0015.png', 'images/legal/3374686/3374686_page_0016.png', 'images/legal/3374686/3374686_page_0017.png', 'images/legal/3374686/3374686_page_0018.png', 'images/legal/3374686/3374686_page_0019.png', 'images/legal/3374686/3374686_page_0020.png', 'images/legal/3374686/3374686_page_0021.png', 'images/legal/3374686/3374686_page_0022.png', 'images/legal/3374686/3374686_page_0023.png']}\n",
      "{'answer': 'Carrier reimbursement: $347,934, Large Deductible Reimbursement: '\n",
      "           '$87, Supplement Benefit Fund: $38,779. In addition, the '\n",
      "           'Association recovered $97,502 from early access distributions, '\n",
      "           'NCCI settlement proceeds, and administrative expense '\n",
      "           'reimbursements.',\n",
      " 'answer_type': 'table_required',\n",
      " 'domain': 'legal',\n",
      " 'gt_image_paths': ['images/legal/3374686/3374686_page_0005.png',\n",
      "                    'images/legal/3374686/3374686_page_0006.png',\n",
      "                    'images/legal/3374686/3374686_page_0007.png'],\n",
      " 'longdoc_image_paths': ['images/legal/3374686/3374686_page_0001.png',\n",
      "                         'images/legal/3374686/3374686_page_0002.png',\n",
      "                         'images/legal/3374686/3374686_page_0003.png',\n",
      "                         'images/legal/3374686/3374686_page_0004.png',\n",
      "                         'images/legal/3374686/3374686_page_0005.png',\n",
      "                         'images/legal/3374686/3374686_page_0006.png',\n",
      "                         'images/legal/3374686/3374686_page_0007.png',\n",
      "                         'images/legal/3374686/3374686_page_0008.png',\n",
      "                         'images/legal/3374686/3374686_page_0009.png',\n",
      "                         'images/legal/3374686/3374686_page_0010.png',\n",
      "                         'images/legal/3374686/3374686_page_0011.png',\n",
      "                         'images/legal/3374686/3374686_page_0012.png',\n",
      "                         'images/legal/3374686/3374686_page_0013.png',\n",
      "                         'images/legal/3374686/3374686_page_0014.png',\n",
      "                         'images/legal/3374686/3374686_page_0015.png',\n",
      "                         'images/legal/3374686/3374686_page_0016.png',\n",
      "                         'images/legal/3374686/3374686_page_0017.png',\n",
      "                         'images/legal/3374686/3374686_page_0018.png',\n",
      "                         'images/legal/3374686/3374686_page_0019.png',\n",
      "                         'images/legal/3374686/3374686_page_0020.png',\n",
      "                         'images/legal/3374686/3374686_page_0021.png',\n",
      "                         'images/legal/3374686/3374686_page_0022.png',\n",
      "                         'images/legal/3374686/3374686_page_0023.png'],\n",
      " 'question': 'In 2020, how much did the Maine Insurance Guaranty Association '\n",
      "             'collect from each recovery category?',\n",
      " 'question_type': 'factual_retrieval'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pprint\n",
    "# Load the dataset from Hugging Face\n",
    "# load a specific domain directly\n",
    "domain_name=\"legal\"\n",
    "\n",
    "dataset = load_dataset(\"Salesforce/UniDoc-Bench\", split=domain_name)\n",
    "\n",
    "# Explore the dataset\n",
    "print(dataset)\n",
    "print(dataset[0])\n",
    "\n",
    "# print more beautifully\n",
    "\n",
    "pprint.pprint(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cdd14db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3698/3736501997.py:30: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(domain_dir)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted legal PDFs to pdf_database/legal\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import tarfile\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Method 1: Download specific PDF archives\n",
    "def download_pdf_archive(domain_name, save_path=\".\"):\n",
    "    \"\"\"Download PDF archive for a specific domain\"\"\"\n",
    "    filename = f\"{domain_name}_pdfs.tar.gz\"\n",
    "    file_path = hf_hub_download(\n",
    "        repo_id=\"Salesforce/UniDoc-Bench\",\n",
    "        filename=filename,\n",
    "        repo_type=\"dataset\",\n",
    "        local_dir=save_path\n",
    "    )\n",
    "    return file_path\n",
    "\n",
    "# Method 2: Extract PDFs for RAG database construction\n",
    "def extract_pdfs_for_rag(domain_name, extract_to=\"pdf_database\"):\n",
    "    \"\"\"Download and extract PDFs for RAG database\"\"\"\n",
    "    # Download the archive\n",
    "    archive_path = download_pdf_archive(domain_name)\n",
    "    \n",
    "    # Create extraction directory\n",
    "    domain_dir = os.path.join(extract_to, domain_name)\n",
    "    os.makedirs(domain_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract PDFs\n",
    "    with tarfile.open(archive_path, 'r:gz') as tar:\n",
    "        tar.extractall(domain_dir)\n",
    "    \n",
    "    print(f\"Extracted {domain_name} PDFs to {domain_dir}\")\n",
    "    return domain_dir\n",
    "\n",
    "# Example: Extract legal PDFs for RAG\n",
    "pdfs = extract_pdfs_for_rag(domain_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb886b2",
   "metadata": {},
   "source": [
    "### Deleting macOS files to liberate space\n",
    "Do not use if you are not on macOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b4ee4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted all macOS files from pdf_database/legal\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def delete_macos_files(domain_name):\n",
    "    \"\"\"Delete macOS specific files from the extracted PDFs\"\"\"\n",
    "    temp_dir = f\"pdf_database/{domain_name}\"\n",
    "    \n",
    "    for root, dirs, files in os.walk(temp_dir):\n",
    "        # Delete macOS directories\n",
    "        if \"__MACOSX\" in dirs:\n",
    "            os.system(f\"rm -rf '{os.path.join(root, '__MACOSX')}'\")\n",
    "        \n",
    "        # Delete macOS files\n",
    "        for file in files:\n",
    "            if file.startswith(\"._\"):\n",
    "                os.remove(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"Deleted all macOS files from {temp_dir}\")\n",
    "\n",
    "delete_macos_files(domain_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8bb594",
   "metadata": {},
   "source": [
    "### Create rag database\n",
    "```python\n",
    "entry = {\n",
    "                \"source\": str(pdf_file),\n",
    "                \"page\": page_num,\n",
    "                \"text\": text,\n",
    "                \"images\": len(images),\n",
    "                \"domain\": domain_name\n",
    "            }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14911bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: format error: cmsOpenProfileFromMem failed\n",
      "\n",
      "Built RAG database for legal: 7624 entries\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import datetime\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.utils.export import generate_multimodal_pages\n",
    "from docling.utils.utils import create_hash\n",
    "\n",
    "_log = logging.getLogger(__name__)\n",
    "\n",
    "IMAGE_RESOLUTION_SCALE = 2.0\n",
    "\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    data_folder = Path(__file__).parent / \"../../tests/data\"\n",
    "    input_doc_path = data_folder / \"pdf/2206.01062.pdf\"\n",
    "    output_dir = Path(\"scratch\")\n",
    "\n",
    "    # Keep page images so they can be exported to the multimodal rows.\n",
    "    # Use PdfPipelineOptions.images_scale to control the render scale (1 ~ 72 DPI).\n",
    "    pipeline_options = PdfPipelineOptions()\n",
    "    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n",
    "    pipeline_options.generate_page_images = True\n",
    "\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    conv_res = doc_converter.convert(input_doc_path)\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    rows = []\n",
    "    for (\n",
    "        content_text,\n",
    "        content_md,\n",
    "        content_dt,\n",
    "        page_cells,\n",
    "        page_segments,\n",
    "        page,\n",
    "    ) in generate_multimodal_pages(conv_res):\n",
    "        dpi = page._default_image_scale * 72\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"document\": conv_res.input.file.name,\n",
    "                \"hash\": conv_res.input.document_hash,\n",
    "                \"page_hash\": create_hash(\n",
    "                    conv_res.input.document_hash + \":\" + str(page.page_no - 1)\n",
    "                ),\n",
    "                \"image\": {\n",
    "                    \"width\": page.image.width,\n",
    "                    \"height\": page.image.height,\n",
    "                    \"bytes\": page.image.tobytes(),\n",
    "                },\n",
    "                \"cells\": page_cells,\n",
    "                \"contents\": content_text,\n",
    "                \"contents_md\": content_md,\n",
    "                \"contents_dt\": content_dt,\n",
    "                \"segments\": page_segments,\n",
    "                \"extra\": {\n",
    "                    \"page_num\": page.page_no + 1,\n",
    "                    \"width_in_points\": page.size.width,\n",
    "                    \"height_in_points\": page.size.height,\n",
    "                    \"dpi\": dpi,\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Generate one parquet from all documents\n",
    "    df_result = pd.json_normalize(rows)\n",
    "    now = datetime.datetime.now()\n",
    "    output_filename = output_dir / f\"multimodal_{now:%Y-%m-%d_%H%M%S}.parquet\"\n",
    "    df_result.to_parquet(output_filename)\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    _log.info(\n",
    "        f\"Document converted and multimodal pages generated in {end_time:.2f} seconds.\"\n",
    "    )\n",
    "\n",
    "    # This block demonstrates how the file can be opened with the HF datasets library\n",
    "    # from datasets import Dataset\n",
    "    # from PIL import Image\n",
    "    # multimodal_df = pd.read_parquet(output_filename)\n",
    "\n",
    "    # # Convert pandas DataFrame to Hugging Face Dataset and load bytes into image\n",
    "    # dataset = Dataset.from_pandas(multimodal_df)\n",
    "    # def transforms(examples):\n",
    "    #     examples[\"image\"] = Image.frombytes('RGB', (examples[\"image.width\"], examples[\"image.height\"]), examples[\"image.bytes\"], 'raw')\n",
    "    #     return examples\n",
    "    # dataset = dataset.map(transforms)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
