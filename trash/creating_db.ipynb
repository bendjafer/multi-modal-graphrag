{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e1f8a3",
   "metadata": {},
   "source": [
    "### Hugging Face Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b5ee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install easyocr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05660396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import notebook_login\n",
    "#access_token = \"\"\n",
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff7c92f",
   "metadata": {},
   "source": [
    "### Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654adca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GraphRAG PDF Processing Module (Optimized + Full Functionality)\n",
    "\n",
    "Handles PDF processing using Docling with EasyOCR. \n",
    "Improved with:\n",
    "1. Content-based hashing (deduplication)\n",
    "2. Visual noise filtering (removes icons/lines)\n",
    "3. Native markdown page slicing (efficient O(N) extraction)\n",
    "\"\"\"\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "import io\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, EasyOcrOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "# Configuration Constants\n",
    "DEFAULT_IMAGE_SCALE = 2.0\n",
    "DEFAULT_OCR_LANGUAGE = [\"en\", \"fr\"]\n",
    "MIN_IMAGE_WIDTH = 120   # Threshold to ignore icons/logos\n",
    "MIN_IMAGE_HEIGHT = 80   # Threshold to ignore horizontal lines\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class PDFProcessingError(Exception):\n",
    "    \"\"\"Custom exception for PDF processing failures.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def setup_converter(\n",
    "    image_scale: float = DEFAULT_IMAGE_SCALE,\n",
    "    extract_tables: bool = True,\n",
    "    extract_figures: bool = True,\n",
    "    extract_images: bool = False,\n",
    ") -> DocumentConverter:\n",
    "    \"\"\"Configures the Docling converter with your original parameters.\"\"\"\n",
    "    try:\n",
    "        pipeline_options = PdfPipelineOptions()\n",
    "        pipeline_options.images_scale = image_scale\n",
    "        pipeline_options.generate_page_images = extract_images\n",
    "        pipeline_options.generate_table_images = extract_tables\n",
    "        pipeline_options.generate_picture_images = extract_figures\n",
    "        \n",
    "        pipeline_options.do_ocr = True\n",
    "        pipeline_options.ocr_options = EasyOcrOptions(lang=DEFAULT_OCR_LANGUAGE)\n",
    "        \n",
    "        return DocumentConverter(\n",
    "            format_options={\n",
    "                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise PDFProcessingError(f\"Failed to initialize converter: {e}\")\n",
    "\n",
    "\n",
    "def _get_image_hash(pil_img) -> str:\n",
    "    \"\"\"Generates a stable hash based on image pixel data.\"\"\"\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_img.save(img_byte_arr, format='PNG')\n",
    "    return hashlib.md5(img_byte_arr.getvalue()).hexdigest()[:16]\n",
    "\n",
    "\n",
    "def _save_image(pil_img, output_path: Path) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Save PIL image and return dimensions.\"\"\"\n",
    "    try:\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        pil_img.save(output_path, format='PNG', optimize=True)\n",
    "        return {\"width\": pil_img.width, \"height\": pil_img.height}\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to save image to {output_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_pdf(\n",
    "    input_path: Path,\n",
    "    output_dir: Optional[Path] = None,\n",
    "    image_scale: float = DEFAULT_IMAGE_SCALE,\n",
    "    extract_tables: bool = True,\n",
    "    extract_figures: bool = True,\n",
    "    extract_images: bool = False,\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Main entry point for PDF processing.\n",
    "    Restores original parameters while fixing logic flaws.\n",
    "    \"\"\"\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {input_path}\")\n",
    "    \n",
    "    # Setup Paths\n",
    "    output_dir = output_dir or input_path.parent.parent / f\"{input_path.stem}_output\"\n",
    "    images_dir = output_dir / \"images\" / input_path.stem\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        converter = setup_converter(image_scale, extract_tables, extract_figures, extract_images)\n",
    "        logger.info(f\"Converting: {input_path.name}...\")\n",
    "        conv_result = converter.convert(input_path)\n",
    "        doc = conv_result.document\n",
    "        \n",
    "        # --- 1. Map Figures/Tables to Pages (Single Pass) ---\n",
    "        page_assets = {}\n",
    "        \n",
    "        # Combine all structural elements\n",
    "        elements = []\n",
    "        if extract_figures:\n",
    "            elements.extend([(p, \"figure\") for p in getattr(doc, 'pictures', [])])\n",
    "        if extract_tables:\n",
    "            elements.extend([(t, \"table\") for t in getattr(doc, 'tables', [])])\n",
    "\n",
    "        for idx, (el, el_type) in enumerate(elements):\n",
    "            pil_img = el.image.pil_image\n",
    "            \n",
    "            # Filter noise unless it's a table (tables usually important regardless of size)\n",
    "            if el_type == \"figure\" and (pil_img.width < MIN_IMAGE_WIDTH or pil_img.height < MIN_IMAGE_HEIGHT):\n",
    "                continue\n",
    "\n",
    "            page_idx = el.prov[0].page_no + 1 if el.prov else 1\n",
    "            img_hash = _get_image_hash(pil_img)\n",
    "            \n",
    "            filename = f\"page_{page_idx:03d}_{el_type}_{idx}_{img_hash}.png\"\n",
    "            target_path = images_dir / filename\n",
    "            \n",
    "            if size := _save_image(pil_img, target_path):\n",
    "                asset_data = {\n",
    "                    \"id\": img_hash,\n",
    "                    \"filename\": filename,\n",
    "                    \"filepath\": str(target_path),\n",
    "                    \"description\": getattr(el, 'caption', f\"{el_type.capitalize()} {idx+1}\"),\n",
    "                    \"type\": el_type,\n",
    "                    \"size\": size\n",
    "                }\n",
    "                page_assets.setdefault(page_idx, []).append(asset_data)\n",
    "\n",
    "        # --- 2. Construct Page Data with Full-Page Image support ---\n",
    "        pages_data = []\n",
    "        for i, page_obj in enumerate(conv_result.pages):\n",
    "            page_num = i + 1\n",
    "            current_page_images = page_assets.get(page_num, [])\n",
    "\n",
    "            # Handle optional full-page screenshots\n",
    "            if extract_images and hasattr(page_obj, 'image') and page_obj.image:\n",
    "                p_pil = page_obj.image.pil_image\n",
    "                p_hash = _get_image_hash(p_pil)\n",
    "                p_filename = f\"page_{page_num:03d}_full_{p_hash}.png\"\n",
    "                p_path = images_dir / p_filename\n",
    "                \n",
    "                if p_size := _save_image(p_pil, p_path):\n",
    "                    current_page_images.insert(0, {\n",
    "                        \"id\": p_hash,\n",
    "                        \"filename\": p_filename,\n",
    "                        \"filepath\": str(p_path),\n",
    "                        \"description\": f\"Full page {page_num} scan\",\n",
    "                        \"type\": \"page_scan\",\n",
    "                        \"size\": p_size\n",
    "                    })\n",
    "\n",
    "            pages_data.append({\n",
    "                \"page_num\": page_num,\n",
    "                \"markdown\": doc.export_to_markdown(page_no=i), # Fixed O(N) extraction\n",
    "                \"images\": current_page_images\n",
    "            })\n",
    "\n",
    "        # --- 3. Final Result & Cleanup ---\n",
    "        result = {\n",
    "            \"document_id\": conv_result.input.document_hash,\n",
    "            \"filename\": input_path.name,\n",
    "            \"total_pages\": len(pages_data),\n",
    "            \"pages\": pages_data\n",
    "        }\n",
    "\n",
    "        # Save JSON\n",
    "        with open(output_dir / f\"{input_path.stem}_result.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        logger.info(f\"Success. Processed {len(pages_data)} pages.\")\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Processing failed for {input_path.name}: {e}\")\n",
    "        raise PDFProcessingError(f\"Pipeline failure: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GraphRAG PDF Processing Module (Optimized + Full Functionality)\n",
    "\n",
    "Handles PDF processing using Docling with EasyOCR. \n",
    "Improved with:\n",
    "1. Content-based hashing (deduplication)\n",
    "2. Visual noise filtering (removes icons/lines)\n",
    "3. Native markdown page slicing (efficient O(N) extraction)\n",
    "\"\"\"\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "import io\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, EasyOcrOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "# Configuration Constants\n",
    "DEFAULT_IMAGE_SCALE = 2.0\n",
    "DEFAULT_OCR_LANGUAGE = [\"en\", \"fr\"]\n",
    "MIN_IMAGE_WIDTH = 120   # Threshold to ignore icons/logos\n",
    "MIN_IMAGE_HEIGHT = 80   # Threshold to ignore horizontal lines\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# EasyOCR/Docling sets pin_memory=True by default (assuming GPU might be available)\n",
    "# I chose to disable this warning to avoid unnecessary noise in the logs. \n",
    "warnings.filterwarnings('ignore', message='.*pin_memory.*')\n",
    "\n",
    "\n",
    "\n",
    "class PDFProcessingError(Exception):\n",
    "    \"\"\"Custom exception for PDF processing failures.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def setup_converter(\n",
    "    image_scale: float = DEFAULT_IMAGE_SCALE,\n",
    "    extract_tables: bool = True,\n",
    "    extract_figures: bool = True,\n",
    "    extract_images: bool = False,\n",
    ") -> DocumentConverter:\n",
    "    \"\"\"Configures the Docling converter with your original parameters.\"\"\"\n",
    "    try:\n",
    "        pipeline_options = PdfPipelineOptions()\n",
    "        pipeline_options.images_scale = image_scale\n",
    "        pipeline_options.generate_page_images = extract_images\n",
    "        pipeline_options.generate_table_images = extract_tables\n",
    "        pipeline_options.generate_picture_images = extract_figures\n",
    "        \n",
    "        pipeline_options.do_ocr = True\n",
    "        pipeline_options.ocr_options = EasyOcrOptions(lang=DEFAULT_OCR_LANGUAGE)\n",
    "        \n",
    "        return DocumentConverter(\n",
    "            format_options={\n",
    "                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise PDFProcessingError(f\"Failed to initialize converter: {e}\")\n",
    "\n",
    "\n",
    "def _get_image_hash(pil_img) -> str:\n",
    "    \"\"\"Generates a stable hash based on image pixel data.\"\"\"\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_img.save(img_byte_arr, format='PNG')\n",
    "    return hashlib.md5(img_byte_arr.getvalue()).hexdigest()[:16]\n",
    "\n",
    "\n",
    "def _save_image(pil_img, output_path: Path) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Save PIL image and return dimensions.\"\"\"\n",
    "    try:\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        pil_img.save(output_path, format='PNG', optimize=True)\n",
    "        return {\"width\": pil_img.width, \"height\": pil_img.height}\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to save image to {output_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_pdf(\n",
    "    input_path: Path,\n",
    "    output_dir: Optional[Path] = None,\n",
    "    image_scale: float = DEFAULT_IMAGE_SCALE,\n",
    "    extract_tables: bool = True,\n",
    "    extract_figures: bool = True,\n",
    "    extract_images: bool = False,\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Main entry point for PDF processing.\n",
    "    Restores original parameters while fixing logic flaws.\n",
    "    \"\"\"\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {input_path}\")\n",
    "    \n",
    "    # Setup Paths\n",
    "    output_dir = output_dir or input_path.parent.parent / f\"{input_path.stem}_output\"\n",
    "    images_dir = output_dir / \"images\" / input_path.stem\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        converter = setup_converter(image_scale, extract_tables, extract_figures, extract_images)\n",
    "        logger.info(f\"Converting: {input_path.name}...\")\n",
    "        conv_result = converter.convert(input_path)\n",
    "        doc = conv_result.document\n",
    "\n",
    "        full_markdown=\"\"\n",
    "        \n",
    "        # --- 1. Map Figures/Tables to Pages (Single Pass) ---\n",
    "        page_assets = {}\n",
    "        \n",
    "        # Combine all structural elements\n",
    "        elements = []\n",
    "        if extract_figures:\n",
    "            elements.extend([(p, \"figure\") for p in getattr(doc, 'pictures', [])])\n",
    "        if extract_tables:\n",
    "            elements.extend([(t, \"table\") for t in getattr(doc, 'tables', [])])\n",
    "\n",
    "        for idx, (el, el_type) in enumerate(elements):\n",
    "            pil_img = el.image.pil_image\n",
    "            \n",
    "            # Filter noise unless it's a table (tables usually important regardless of size)\n",
    "            if el_type == \"figure\" and (pil_img.width < MIN_IMAGE_WIDTH or pil_img.height < MIN_IMAGE_HEIGHT):\n",
    "                continue\n",
    "\n",
    "            page_idx = el.prov[0].page_no + 1 if el.prov else 1\n",
    "            img_hash = _get_image_hash(pil_img)\n",
    "            \n",
    "            filename = f\"page_{page_idx:03d}_{el_type}_{idx}_{img_hash}.png\"\n",
    "            target_path = images_dir / filename\n",
    "            \n",
    "            if size := _save_image(pil_img, target_path):\n",
    "                asset_data = {\n",
    "                    \"id\": img_hash,\n",
    "                    \"filename\": filename,\n",
    "                    \"filepath\": str(target_path),\n",
    "                    \"description\": getattr(el, 'caption', f\"{el_type.capitalize()} {idx+1}\"),\n",
    "                    \"type\": el_type,\n",
    "                    \"size\": size\n",
    "                }\n",
    "                page_assets.setdefault(page_idx, []).append(asset_data)\n",
    "\n",
    "        # --- 2. Construct Page Data with Full-Page Image support ---\n",
    "        pages_data = []\n",
    "        for i, page_obj in enumerate(conv_result.pages):\n",
    "            page_num = i + 1\n",
    "            current_page_images = page_assets.get(page_num, [])\n",
    "\n",
    "            # Handle optional full-page screenshots\n",
    "            if extract_images and hasattr(page_obj, 'image') and page_obj.image:\n",
    "                p_pil = page_obj.image.pil_image\n",
    "                p_hash = _get_image_hash(p_pil)\n",
    "                p_filename = f\"page_{page_num:03d}_full_{p_hash}.png\"\n",
    "                p_path = images_dir / p_filename\n",
    "                \n",
    "                if p_size := _save_image(p_pil, p_path):\n",
    "                    current_page_images.insert(0, {\n",
    "                        \"id\": p_hash,\n",
    "                        \"filename\": p_filename,\n",
    "                        \"filepath\": str(p_path),\n",
    "                        \"description\": f\"Full page {page_num} scan\",\n",
    "                        \"type\": \"page_scan\",\n",
    "                        \"size\": p_size\n",
    "                    })\n",
    "            markdown_page_results=doc.export_to_markdown(page_no=i)\n",
    "            full_markdown+=markdown_page_results\n",
    "            pages_data.append({\n",
    "                \"page_num\": page_num,\n",
    "                \"markdown\": markdown_page_results, # Fixed O(N) extraction\n",
    "                \"images\": current_page_images\n",
    "            })\n",
    "\n",
    "        # --- 3. Final Result & Cleanup ---\n",
    "        result = {\n",
    "            \"document_id\": conv_result.input.document_hash,\n",
    "            \"filename\": input_path.name,\n",
    "            \"full_markdown\":full_markdown,\n",
    "            \"total_pages\": len(pages_data),\n",
    "            \"pages\": pages_data\n",
    "        }\n",
    "\n",
    "        # Save JSON\n",
    "        with open(output_dir / f\"{input_path.stem}_result.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        logger.info(f\"Success. Processed {len(pages_data)} pages.\")\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Processing failed for {input_path.name}: {e}\")\n",
    "        raise PDFProcessingError(f\"Pipeline failure: {e}\")\n",
    "    \n",
    "\n",
    "\n",
    "pdf_path = Path(\"dummy_pdfs/0010514.pdf\")\n",
    "\n",
    "try:\n",
    "    result = process_pdf(pdf_path, image_scale=2.0, extract_tables=True, extract_figures=True, extract_images=False)\n",
    "    if result:\n",
    "        print(f\"Processed {result['total_pages']} pages\")\n",
    "        print(f\"Document hash: {result['document_hash']}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Processing failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf395d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Converting: 0010514.pdf...\n",
      "INFO:docling.datamodel.document:detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.document_converter:Initializing pipeline for StandardPdfPipeline with options hash eb35847499f704a2741279d41d6d5e6c\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'\n",
      "INFO:docling.pipeline.base_pipeline:Processing document 0010514.pdf\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/houssam/miniconda3/envs/rag/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "INFO:docling.document_converter:Finished converting document 0010514.pdf in 160.49 sec.\n",
      "INFO:__main__:Success. Processed 14 pages.\n",
      "ERROR:__main__:Processing failed: 'document_hash'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 14 pages\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30705a91",
   "metadata": {},
   "source": [
    "### Setup Converter Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11a8cd0",
   "metadata": {},
   "source": [
    "### Process PDF Function\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"document_id\": \"hash\",\n",
    "    \"filename\": \"pdf_name.pdf\",\n",
    "    \"total_pages\": int,\n",
    "    \"pages\": [\n",
    "        {\n",
    "            \"page_num\": 1,\n",
    "            \"markdown\": \"...\",\n",
    "            \"images\": [\n",
    "                {\n",
    "                    \"id\": \"hash\",\n",
    "                    \"filename\": \"page_001_img_0.png\",\n",
    "                    \"filepath\": \"images/pdf_name/page_001_img_0.png\",\n",
    "                    \"description\": \"Figure description\",\n",
    "                    \"size\": {\"width\": int, \"height\": int}\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
